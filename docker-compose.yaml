services:
  stt:
    build:
      context: .
      dockerfile: dockerfile
    container_name: STT
    runtime: nvidia
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}
    ports:
      - "8002:8000"
    ipc: host
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    command:
      - --gpu_memory_utilization=0.41 # 41
      - --model
      - openai/whisper-large-v3-turbo
      - --task
      - transcription
      - --max-model-len=50 #  Each sequence can be up to max_model_len tokens
      # long how much seconds of audio can be processed in one go 
      # Maximum concurrency for 100 tokens per request: 44.96x
      - --max-num-seqs=2 # This reduces the number of concurrent requests in a batch, 
      # thereby requiring less KV cache space.
      #- --enforce-eager

# Maximum concurrency for 448 tokens per request: 10.04x 448
# Maximum concurrency for 100 tokens per request: 44.96x 100
#  Maximum concurrency for 50 tokens per request: 89.92x 50